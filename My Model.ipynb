{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "the MNIST dataset is a database of 60,000 training images and 10,000 test images of handwritten digits that is commonly used for training and testing in the field of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing neccesary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is available in Keras' built in 'dataset' library already, meaning we simply need to import it. We will also import a number of other libraries such as numpy, matplotlib and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as kr\n",
    "#from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training and test dataset\n",
    "MNIST will give us two tuples. The first tuple is the training set and the second is the test set.\n",
    "\n",
    "\n",
    "#### Now we should normalise the data\n",
    "\n",
    "This basically means we want to scale it down between 0 and 1 which makes it easier for the network to process. It's important to note that the distance between each value is still the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = mnist.load_data() # Load datasets\n",
    "\n",
    "X_train = kr.utils.normalize(X_train, axis=1) # Normalize x train and test sets\n",
    "X_test = kr.utils.normalize(X_train, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the shape of each dataset\n",
    "We can print the shape of the data in each dataset to confirm the number of images and the shape of each image. We see here that we have 60'000 images in our training set and each image is 28 pixels x 28. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the first image of the X_train dataset\n",
    "Now we can plot the first image of the x_train dataset using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, ' Digit 5')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], plt.cm.binary) # this is not a color image so we use colormap to change it to black and white\n",
    "plt.title(\" Digit \" + str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is an array of the actual data we want to pass through our neural network.\n",
    "\n",
    "Below, the 0's in the array repersent the blank space in the image and the numbers ranging from 0.01-0.99 represent the pencil stroke of the digit.\n",
    "\n",
    "Before normalisation these numbers would have ranged between 0-255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00393124, 0.02332955, 0.02620568,\n",
       "        0.02625207, 0.17420356, 0.17566281, 0.28629534, 0.05664824,\n",
       "        0.51877786, 0.71632322, 0.77892406, 0.89301644, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05780486, 0.06524513,\n",
       "        0.16128198, 0.22713296, 0.22277047, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.32678448, 0.368094  , 0.3747499 ,\n",
       "        0.79066747, 0.67980478, 0.61494005, 0.45002403, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.12250613, 0.45858525, 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.32420121, 0.15214552, 0.17865984,\n",
       "        0.25626376, 0.1573102 , 0.12298801, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04500225, 0.4219755 , 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.32790981, 0.28826244,\n",
       "        0.26543758, 0.34149427, 0.31128482, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.1541463 , 0.28272888,\n",
       "        0.18358693, 0.37314701, 0.33153488, 0.26569767, 0.01601458,\n",
       "        0.        , 0.05945042, 0.19891229, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0253731 ,\n",
       "        0.00171577, 0.22713296, 0.33153488, 0.11664776, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20500962, 0.33153488, 0.24625638, 0.00291174,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01622378, 0.24897876, 0.32790981, 0.10191096,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04586451, 0.31235677, 0.32757096,\n",
       "        0.23335172, 0.14931733, 0.00129164, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10498298, 0.34940902,\n",
       "        0.3689874 , 0.34978968, 0.15370495, 0.04089933, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06551419,\n",
       "        0.27127137, 0.34978968, 0.32678448, 0.245396  , 0.05882702,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02333517, 0.12857881, 0.32549285, 0.41390126, 0.40743158,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32161793, 0.41390126, 0.54251585,\n",
       "        0.20001074, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06697006,\n",
       "        0.18959827, 0.25300993, 0.32678448, 0.41390126, 0.45100715,\n",
       "        0.00625034, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.05110617, 0.19182076, 0.33339444,\n",
       "        0.3689874 , 0.34978968, 0.32678448, 0.40899334, 0.39653769,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04117838, 0.16813739, 0.28960162, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.25961929, 0.12760592, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04431706, 0.11961607,\n",
       "        0.36545809, 0.37314701, 0.33153488, 0.32790981, 0.36833534,\n",
       "        0.28877275, 0.111988  , 0.00258328, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05298497, 0.42752138, 0.4219755 , 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.25273681, 0.11646967,\n",
       "        0.01312603, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.37491383,\n",
       "        0.56222061, 0.66525569, 0.63253163, 0.48748768, 0.45852825,\n",
       "        0.43408872, 0.359873  , 0.17428513, 0.01425695, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.92705966,\n",
       "        0.82698729, 0.74473314, 0.63253163, 0.4084877 , 0.24466922,\n",
       "        0.22648107, 0.02359823, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are ready to build our model\n",
    "\n",
    "A sequential model is a linear stack of layers. We make our input layer flatten the data being passed to it. This is neccesary because our images are 28x28 multi-dimensional arrays which we do not want.\n",
    "\n",
    "Our next two layers are dense layers. A dense layer is just a regular layer of neurons in a neural network. Each neuron recieves input from all the neurons in the previous layer, thus densely connected. We pass them two parameters; how many neurons in the layer and the activasion function.\n",
    "\n",
    "Our final layer has 10 neurons specified as our input data ranges from 0-9 and our activation function is softmax for probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential() # Create a new sequential neural network\n",
    "model.add(kr.layers.Flatten()) # Input layer\n",
    "model.add(kr.layers.Dense(128, activation=\"relu\")) # 128 neurons and the 'basic' activation function.\n",
    "model.add(kr.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(kr.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and train our model\n",
    "Here we pass 3 parameters; the optimizer to use, the loss metric (the degree of error) and the metrics to track.\n",
    "\n",
    "To train the model we use the fit function. We pass in what we want trained and the epochs. The epochs is simply how many times we want our neural network to go over the training data set. If we set it to 1, the neural network will get to see the data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.2691 - acc: 0.9206\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.1200 - acc: 0.9636\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0895 - acc: 0.9734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15600d788d0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"]) # Played around with 'sgd' and 'adam' optimizer also.\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate validation loss and accuracy\n",
    "Next we should calculate our validatio loss and accuracy. This is where the test sets come in. To do this we use the evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 60000 input samples and 10000 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-cb2a49bac2e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1286\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1289\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m                 \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m                 \u001b[1;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    238\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 60000 input samples and 10000 target samples."
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(X_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(1, 101)\n",
    "    \n",
    "np.argmax(predictions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[i])\n",
    "#plt.title(\" Digit \" + str(y_test[5]) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A summary of the model\n",
    "We can output a summary of the model we have built with the summary function. This function gives us the layers in our model and their shape as well as parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_22 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
